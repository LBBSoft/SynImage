# -*- coding: utf-8 -*-
"""SynImage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14t8ywOpBaC_HuDTdnfVgBWzOetmLzDag
"""

###Set up the GPU environment with CUDA 10.1 and create a new conda environment named 'ImageMol' with Python 3.7.3.
###Install necessary packages including RDKit, PyTorch, and torchvision, tailored for CUDA 10.1.
###Downloaded pre-trained ImageMol and Utilized the ImageMol pre-trained model from GitHub Repository "https://github.com/HongxinXiang/ImageMol"  to extract features from SMILES drugs in 'drugsSMILE_ONEIL.csv'.
### Saved the extracted features into 'ImageMol_ONEILdrugs_features.csv' for further analysis.



import torch
import torch.nn as nn
import torch.utils.data as Data
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import torch.optim as optim
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageFont, ImageDraw
import random
from sklearn.model_selection import KFold
from scipy.stats import pearsonr
from sklearn.preprocessing import normalize
import sklearn.preprocessing as skpre
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
from numpy.ma.core import exp
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score, precision_score, cohen_kappa_score
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import mean_squared_error, r2_score

def clssification(y_true, y_pred, threshold):

    y_true = np.array(list(map(lambda x : 1 if x >= threshold[1] else x, y_true)))
    y_true = np.array(list(map(lambda x : 0 if x < threshold[0] else x, y_true)))

    y_pred = np.array(list(map(lambda x : 1 if x >= threshold[1] else x, y_pred)))
    y_pred = np.array(list(map(lambda x : 0 if x < threshold[0] else x, y_pred)))

    _yt = []
    _yp = []
    for a, b in zip(y_true, y_pred):
        if (a==0 or a==1) and (b==0 or b==1):
            _yt.append(a)
            _yp.append(b)
    y_true = np.array(_yt)
    y_pred = np.array(_yp)

    acc = accuracy_score(y_true, y_pred)
    kappa = cohen_kappa_score(y_true, y_pred)
    bacc = balanced_accuracy_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)

    print(f"| roc_auc: {roc_auc} |")
    print(f"| acc: {acc} |")
    print(f"| bacc: {bacc} |")
    print(f"| prec: {prec} |")
    print(f"| kappa: {kappa} |")

# Load the ONEIL dataset
data_ONEIL = pd.read_csv('/path/Data/data_ONEIL.csv')

# Load the ResNet18 cell features
cline_feature = pd.read_csv('/path/Data/Resnet18_cell_features.csv')

# Load the ImageMol drug features
d1 = pd.read_csv('/path/Data/ImageMol_ONEILdrugs_features.csv')

# Load the drug SMILES data
d2 = pd.read_csv('/path/Data/drugsSMILE_ONEIL.csv')

# Concatenate the drug names and ImageMol drug features
drug_feature = pd.concat([d2['dname'], d1], axis=1)

# Set the 'name' column as the index for the cell line features DataFrame
cline_feature = cline_feature.set_index(['name'])

# Create a mapping dictionary from cell line names to integer indices
c_map = dict(zip(cline_feature.index, range(34)))

# Set the 'dname' column as the index for the drug features DataFrame
drug_feature = drug_feature.set_index('dname')

# Create a mapping dictionary from drug names to integer indices
d_map = dict(zip(drug_feature.index, range(38)))

# Select specific columns from the ONEIL dataset
data_ONEIL = data_ONEIL[['drug_row', 'drug_col', 'cell_line_name', 'synergy_loewe']]

# Create a list of tuples containing mapped drug and cell line indices and synergy scores
syn = [[d_map[str(row[0])], d_map[str(row[1])], c_map[row[2]], float(row[3])]
        for index, row in data_ONEIL.iterrows()
        if (str(row[0]) in drug_feature.index and str(row[1]) in drug_feature.index and
            str(row[2]) in cline_feature.index)]

# Convert the cell line features to a NumPy array
cline_feature = np.asarray(cline_feature)

# Convert the drug features to a NumPy array
drug_feature = np.asarray(drug_feature)

# Convert the cell line features from NumPy array to PyTorch tensor
cline_feature = torch.from_numpy(cline_feature)

# Create a DataLoader for the cell line features
cline_set = Data.DataLoader(dataset=Data.TensorDataset(cline_feature),
                           batch_size=len(cline_feature), shuffle=False)

# Convert the drug features from NumPy array to PyTorch tensor
drug_feature = torch.from_numpy(drug_feature)

# Create a DataLoader for the drug features
drug_set = Data.DataLoader(dataset=Data.TensorDataset(drug_feature),
                          batch_size=len(drug_feature), shuffle=False)

def data_split(synergy, rd_seed=1):
    synergy_pos = pd.DataFrame([i for i in syn])

    # -----split synergy into 5CV,test set
    train_size = 0.9
    synergy_cv_data, synergy_test = np.split(np.array(synergy_pos.sample(frac=1, random_state=rd_seed)),
                                             [int(train_size * len(synergy_pos))])

    np.random.shuffle(synergy_cv_data)
    np.random.shuffle(synergy_test)

    test_label = torch.from_numpy(np.array(synergy_test[:, 3], dtype='float32'))
    test_ind = torch.from_numpy(np.array(synergy_test[:, 0:3])).long()
    return synergy_cv_data, test_ind, test_label

synergy_cv_data, test_ind, test_label=data_split(syn,rd_seed=1)
cv_data=synergy_cv_data

# Initialize lists to store metrics
train_losses = []
val_losses = []
mse_values = []
r2_values = []
pearson_corr_values = []
pearson_pvalue_values = []
roc_auc_values = []
acc_values = []
bacc_values = []
prec_values = []
kappa_values = []
threshold = [-5, 5]



class DrugSynergyModel(nn.Module):
    def __init__(self, dim_drug, dim_cellline, shared_output_dim=128):
        super(DrugSynergyModel, self).__init__()

        # Shared fully connected layers for drugs
        self.fc_drug_1 = nn.Linear(dim_drug, 512)
        self.fc_drug_2 = nn.Linear(512, 256)
        self.fc_drug_3 = nn.Linear(256, shared_output_dim)

        # Fully connected layers for cell line
        self.fc_cellline_1 = nn.Linear(dim_cellline, 512)
        self.fc_cellline_2 = nn.Linear(512, 256)
        self.fc_cellline_3 = nn.Linear(256, shared_output_dim)

        # Combined fully connected layers
        self.fc_combined_1 = nn.Linear(shared_output_dim * 3, 256)
        self.fc_combined_2 = nn.Linear(256,128)
        self.fc_combined_3 = nn.Linear(128, 32)
        self.fc_combined_4 = nn.Linear(32, 1)
        self.relu = nn.LeakyReLU(0.1)
        self.dropout = nn.Dropout(0.2)

    def forward(self, drug_features, cellline_features, druga_id, drugb_id, cellline_id):
        # Drug 1 forward pass
        drug1 = drug_features[druga_id]
        drug2 = drug_features[drugb_id]

        # Shared layers for both drugs
        x1 = self.relu(self.fc_drug_1(drug1))
        x1 = self.dropout(x1)
        x1 = self.relu(self.fc_drug_2(x1))
        x1 = self.dropout(x1)
        x1=self.relu(self.fc_drug_3(x1))

        x2 = self.relu(self.fc_drug_1(drug2))
        x2 = self.dropout(x2)
        x2 = self.relu(self.fc_drug_2(x2))
        x2 = self.dropout(x2)
        x2=self.relu(self.fc_drug_3(x2))

        # Cell line forward pass
        cellline = cellline_features[cellline_id]
        x3 = self.relu(self.fc_cellline_1(cellline))
        x3 = self.dropout(x3)
        x3 = self.relu(self.fc_cellline_2(x3))
        x3=self.dropout(x3)
        x3=self.relu(self.fc_cellline_3(x3))

        # Concatenate all embeddings
        x = torch.cat((x1, x2, x3), dim=1)

        # Combined fully connected layers
        x = self.relu(self.fc_combined_1(x))
        x = self.dropout(x)
        x = self.relu(self.fc_combined_2(x))
        x = self.dropout(x)
        x=  self.relu(self.fc_combined_3(x))
        x=  self.fc_combined_4(x)
        return x

def train(drug_set, cline_set, index, label, model, loss_func, optimizer):
    model.train()
    optimizer.zero_grad()
    druga_id = index[:, 0]
    drugb_id = index[:, 1]
    cellline_id = index[:, 2]
    outputs = model(drug_set, cline_set, druga_id, drugb_id, cellline_id)
    loss = loss_func(outputs.squeeze(), label)
    loss.backward()
    optimizer.step()
    return loss.item(),outputs

def test(drug_set, cline_set, index, label, model, loss_func):
    model.eval()
    with torch.no_grad():
        druga_id = index[:, 0]
        drugb_id = index[:, 1]
        cellline_id = index[:, 2]
        outputs = model(drug_set, cline_set, druga_id, drugb_id, cellline_id)
        loss = loss_func(outputs.squeeze(), label)
        return loss.item(), outputs

# Data preparation
dim_drug = 512
dim_cellline = 512
shared_output_dim = 128
learning_rate =9e-4
L2 = 1e-4
epochs =1600



kf = KFold(n_splits=5, shuffle=True, random_state=0)
for train_index, validation_index in kf.split(cv_data):
    synergy_train, synergy_validation = cv_data[train_index], cv_data[validation_index]
    label_train = torch.from_numpy(np.array(synergy_train[:, 3], dtype='float32'))
    label_validation = torch.from_numpy(np.array(synergy_validation[:, 3], dtype='float32'))
    index_train = torch.from_numpy(synergy_train[:, 0:3]).long()
    index_validation = torch.from_numpy(synergy_validation[:, 0:3]).long()

    model = DrugSynergyModel(dim_drug, dim_cellline, shared_output_dim)
    loss_func = torch.nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2)

    for epoch in range(epochs):
        train_loss,train_outputs = train(drug_feature, cline_feature, index_train, label_train, model, loss_func, optimizer)
        val_loss, val_outputs = test(drug_feature, cline_feature, index_validation, label_validation, model, loss_func)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        # Calculate additional metrics
        train_mse = mean_squared_error(label_train.detach().numpy(), train_outputs.squeeze().detach().numpy())
        train_rmse = np.sqrt(train_mse)
        train_r2 = r2_score(label_train.detach().numpy(), train_outputs.squeeze().detach().numpy())

        val_mse = mean_squared_error(label_validation.detach().numpy(), val_outputs.squeeze().detach().numpy())
        val_rmse = np.sqrt(val_mse)
        val_r2 = r2_score(label_validation.detach().numpy(), val_outputs.squeeze().detach().numpy())
        train_pearson_corr, train_pearson_pvalue = pearsonr(label_train.detach().numpy(), train_outputs.squeeze().detach().numpy())
        val_pearson_corr, val_pearson_pvalue = pearsonr(label_validation.detach().numpy(), val_outputs.squeeze().detach().numpy())
    # Classification metrics
        roc_auc, acc, bacc, prec, kappa = classification(label_validation.detach().numpy(), val_outputs.squeeze().detach().numpy(), threshold)

        roc_auc_values.append(roc_auc)
        acc_values.append(acc)
        bacc_values.append(bacc)
        prec_values.append(prec)
        kappa_values.append(kappa)print(clssification(label_validation.detach().numpy(), val_outputs.squeeze().detach().numpy(), threshold))

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    mse_values.append(val_mse)
    r2_values.append(val_r2)
    pearson_corr_values.append(val_pearson_corr)
    pearson_pvalue_values.append(val_pearson_pvalue)




# Compute mean and standard deviation for each metric
mean_train_loss = np.mean(train_losses)
std_train_loss = np.std(train_losses)
mean_val_loss = np.mean(val_losses)
std_val_loss = np.std(val_losses)
mean_mse = np.mean(mse_values)
std_mse = np.std(mse_values)
mean_r2 = np.mean(r2_values)
std_r2 = np.std(r2_values)
mean_pearson_corr = np.mean(pearson_corr_values)
std_pearson_corr = np.std(pearson_corr_values)
mean_pearson_pvalue = np.mean(pearson_pvalue_values)
std_pearson_pvalue = np.std(pearson_pvalue_values)
mean_roc_auc = np.mean(roc_auc_values)
std_roc_auc = np.std(roc_auc_values)
mean_acc = np.mean(acc_values)
std_acc = np.std(acc_values)
mean_bacc = np.mean(bacc_values)
std_bacc = np.std(bacc_values)
mean_prec = np.mean(prec_values)
std_prec = np.std(prec_values)
mean_kappa = np.mean(kappa_values)
std_kappa = np.std(kappa_values)

# Print results
print(f'Mean Train Loss: {mean_train_loss:.6f}, Std Train Loss: {std_train_loss:.6f}')
print(f'Mean Validation Loss: {mean_val_loss:.6f}, Std Validation Loss: {std_val_loss:.6f}')
print(f'Mean MSE: {mean_mse:.6f}, Std MSE: {std_mse:.6f}')
print(f'Mean R^2: {mean_r2:.6f}, Std R^2: {std_r2:.6f}')
print(f'Mean Pearson Correlation: {mean_pearson_corr:.6f}, Std Pearson Correlation: {std_pearson_corr:.6f}')
print(f'Mean Pearson p-value: {mean_pearson_pvalue:.6f}, Std Pearson p-value: {std_pearson_pvalue:.6f}')
print(f'Mean ROC AUC: {mean_roc_auc:.6f}, Std ROC AUC: {std_roc_auc:.6f}')
print(f'Mean Accuracy: {mean_acc:.6f}, Std Accuracy: {std_acc:.6f}')
print(f'Mean Balanced Accuracy: {mean_bacc:.6f}, Std Balanced Accuracy: {std_bacc:.6f}')
print(f'Mean Precision: {mean_prec:.6f}, Std Precision: {std_prec:.6f}')
print(f'Mean Cohen Kappa: {mean_kappa:.6f}, Std Cohen Kappa: {std_kappa:.6f}')